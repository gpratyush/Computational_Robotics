{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "class State:\n",
    "    def  __init__(self, x, y, h):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.h = h\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        self.x = state.x\n",
    "        self.y = state.y\n",
    "        self.h = state.h\n",
    "        \n",
    "    def equals(self, new_state):\n",
    "        if self.x == new_state.x and self.y == new_state.y and self.h == new_state.h:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def print_state(self):\n",
    "        print(\"State x,y,h is: \" + str(self.x) + \",\" + str(self.y)+  \",\" + str(self.h))\n",
    "    \n",
    "class Matrix:\n",
    "    def __init__(self, length, width, p_error, x ,y, h):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.p_error = p_error\n",
    "        self.grid = []\n",
    "        self.state = State(x,y,h)\n",
    "        self.vertical_positive = [11,0,1]\n",
    "        self.vertical_negative = [5,6,7]\n",
    "        self.horizontal_positive = [2,3,4]\n",
    "        self.horizontal_negative = [10,9,8]\n",
    "        for i in range(self.width):\n",
    "            temp = [0] * length\n",
    "            self.grid.append(temp)\n",
    "        self.reward_dict = {}\n",
    "    \n",
    "    def get_reward_map(self):\n",
    "        return self.reward_dict\n",
    "            \n",
    "    def set_state(self, new_state):\n",
    "        self.state = new_state\n",
    "    \n",
    "    def get_grid(self):\n",
    "        self.grid[self.state.y][self.state.x] = 1\n",
    "        return np.array(self.grid)\n",
    "\n",
    "    def pre_rotation(self):\n",
    "        v = random.random()\n",
    "        #print(\"The pre rotation probability: \" + str(v))\n",
    "        if v < self.p_error:\n",
    "            return -1\n",
    "        elif v >= self.p_error and v < 2*self.p_error:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def test_move(self, action, error):\n",
    "        h = (self.state.h + error) % 12\n",
    "        multiplier = 1\n",
    "        if h in self.horizontal_negative or h in self.vertical_negative:\n",
    "            multiplier = -1\n",
    "        new_x = self.state.x\n",
    "        new_y = self.state.y\n",
    "        #print(new_x)\n",
    "        #print(new_y)\n",
    "        #print(h)\n",
    "        #print(error)\n",
    "        if h in self.horizontal_negative or h in self.horizontal_positive:\n",
    "            temp_x = self.state.x + (multiplier*action[0])\n",
    "            if 0 <= temp_x <= self.length - 1:\n",
    "                new_x += (multiplier*action[0])\n",
    "        else:\n",
    "            temp_y = self.state.y + (multiplier*action[0])\n",
    "            if 0 <= temp_y <= self.width - 1:\n",
    "                new_y += (multiplier*action[0])\n",
    "        new_h = (h + action[1]) % 12\n",
    "        #print(new_x)\n",
    "        #print(new_y)\n",
    "        #print(new_h)\n",
    "        #print(\"------\")\n",
    "        return State(new_x, new_y, new_h)\n",
    "\n",
    "    def probability_new_state(self, input_action, new_state):\n",
    "        if input_action[0] == 0:\n",
    "            if self.state.equals(new_state):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            probability = 0\n",
    "            errors = [-1, 0, 1]\n",
    "            for e in errors:\n",
    "                result_state = self.test_move(input_action, e)\n",
    "                if new_state.equals(result_state):\n",
    "                    #if new_x == result_state[0] and new_y == result_state[1] and new_h == result_state[2]:\n",
    "                    if e == 1 or e == -1:\n",
    "                        probability += self.p_error\n",
    "                    else:\n",
    "                        probability += (1 - 2*self.p_error)\n",
    "            return probability\n",
    "        \n",
    "    def move(self, action):\n",
    "        h = (self.state.h + self.pre_rotation()) % 12\n",
    "        multiplier = 1\n",
    "        if h in self.horizontal_negative or h in self.vertical_negative:\n",
    "            multiplier = -1\n",
    "        new_x = self.state.x\n",
    "        new_y = self.state.y\n",
    "        if h in self.horizontal_negative or h in self.horizontal_positive:\n",
    "            temp_x = self.state.x + (multiplier*action[0])\n",
    "            if 0 <= temp_x <= self.length - 1:\n",
    "                new_x += (multiplier*action[0])\n",
    "        else:\n",
    "            temp_y = self.state.y + (multiplier*action[0])\n",
    "            if 0 <= temp_y <= self.width - 1:\n",
    "                new_y += (multiplier*action[0])\n",
    "        new_h = (h + action[1]) % 12\n",
    "        return State(new_x, new_y, new_h)\n",
    "        \n",
    "    def get_next_state(self, p_error, initial_state, action):\n",
    "        self.set_state(initial_state)\n",
    "        new_states = []\n",
    "        x,y,h = initial_state.x, initial_state.y, initial_state.h\n",
    "        new_positions = [(x,y), (x-1,y), (x+1,y), (x,y-1), (x,y+1)]\n",
    "        new_headings = [(h-2)%12, (h-1)%12, h, (h+1)%12, (h+2)%12]\n",
    "        for p in new_positions:\n",
    "            for o in new_headings:\n",
    "                new_states.append(State(p[0], p[1], o))\n",
    "        probabilities = []\n",
    "        for s in new_states:\n",
    "            prob = self.probability_new_state(action, s)\n",
    "            probabilities.append(prob)\n",
    "        #print(\"Sum of probabilities is: \" + str(sum(probabilities)))\n",
    "        #print(probabilities)\n",
    "        v = random.random()\n",
    "        #print(v)\n",
    "        cumulative = 0\n",
    "        for i in range(len(probabilities)):\n",
    "            cumulative += probabilities[i]\n",
    "            if v <= cumulative and probabilities[i] != 0:\n",
    "                return new_states[i]\n",
    "    \n",
    "    def get_next_state_probabilities(self, p_error, initial_state, action):\n",
    "        self.p_error = p_error\n",
    "        return_value = []\n",
    "        self.set_state(initial_state)\n",
    "        new_states = []\n",
    "        x,y,h = initial_state.x, initial_state.y, initial_state.h\n",
    "        #pre-rotate left\n",
    "        #ns=State(x,y,np.mod(h-1,12))\n",
    "        \n",
    "        positions = [(x,y), (x-1,y), (x+1,y), (x,y-1), (x,y+1)]\n",
    "        new_positions = []\n",
    "        for p in positions:\n",
    "            if 0 <= p[0] <= self.length - 1 and 0 <= p[1] <= self.width - 1:\n",
    "                new_positions.append((p[0],p[1]))\n",
    "        new_headings = [(h-2)%12, (h-1)%12, h, (h+1)%12, (h+2)%12]\n",
    "        for p in new_positions:\n",
    "            for o in new_headings:\n",
    "                new_states.append(State(p[0], p[1], o))\n",
    "        probabilities = []\n",
    "        for s in new_states:\n",
    "            prob = self.probability_new_state(action, s)\n",
    "            return_value.append((s, prob, self.get_reward(s)))\n",
    "        return return_value\n",
    "        \n",
    "            \n",
    "    def get_reward(self, input_state):\n",
    "        x,y = input_state.x, input_state.y\n",
    "        return self.reward_dict[(x,y)]\n",
    "\n",
    "m = Matrix(8,8,0.0,0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do():\n",
    "    m = Matrix(3, 4, 0.25, 1, 2, 0)\n",
    "    #print(m.get_grid())\n",
    "    '''\n",
    "    action = [1,1]\n",
    "    new_state = State(0,0,9)\n",
    "    p = m.probability_new_state(action, new_state)\n",
    "    print(p)\n",
    "    '''\n",
    "    new_state = m.get_next_state(0.25, State(1,2,0), [1,1])\n",
    "    x,y,h = new_state.x, new_state.y, new_state.h\n",
    "    return x,y,h\n",
    "\n",
    "def main():\n",
    "    m = Matrix(3,4,0.25,1,2,0)\n",
    "    '''\n",
    "    d = {}\n",
    "    for i in range(100000):\n",
    "        h = do()\n",
    "        if h in d:\n",
    "            d[h] += 1\n",
    "        else:\n",
    "            d[h] = 1\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "    print(d)\n",
    "    '''\n",
    "    print(m.get_reward_map(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#This part deals with Question 2 and setting the reward map.\n",
    "def make_reward_map():\n",
    "    d = {}\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if i == 7 or j == 7:\n",
    "                d[(i,j)] = -100\n",
    "            elif i == 0 or j == 0:\n",
    "                d[(i,j)] = -100\n",
    "            elif i == 5 and j == 6:\n",
    "                d[(i,j)] = 1\n",
    "            else:\n",
    "                d[(i,j)] = 0\n",
    "    d[(3,6)] = -10\n",
    "    d[(3,5)] = -10\n",
    "    d[(3,4)] = -10\n",
    "    return d\n",
    "m.reward_dict = make_reward_map()\n",
    "print(m.get_reward(State(0,0,3)))\n",
    "print(m.get_reward(State(6,5,3)))\n",
    "print(m.get_reward(State(4,6,3)))\n",
    "print(m.get_reward(State(5,3,11)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "Foudn goal!\n",
      "State x,y,h is: 3,4,1\n",
      "State x,y,h is: 3,5,2\n",
      "State x,y,h is: 4,5,3\n",
      "State x,y,h is: 5,5,4\n",
      "State x,y,h is: 6,5,5\n",
      "State x,y,h is: 6,6,6\n",
      "State x,y,h is: 6,5,7\n",
      "State x,y,h is: 6,6,8\n",
      "State x,y,h is: 5,6,9\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "#Algorithm for Part 3 initial question. We are using Manhattan distance as the metric. The algorithm just \n",
    "#returns the action which corresponds to the lowest Manhattan distance out of 6 possible actions. Unless the agent\n",
    "#is in the goal state, it must move.\n",
    "\n",
    "goal_state = State(5,6,None)\n",
    "\n",
    "def manhattan_distance(current_state):\n",
    "    return abs(current_state.x - goal_state.x) + abs(current_state.y - goal_state.y)\n",
    "\n",
    "def initial_policy(current_state):\n",
    "    if current_state.x == goal_state.x and current_state.y == goal_state.y:\n",
    "        return [0,0]\n",
    "    actions = ([1,1],[1,0],[1,-1],[-1,1],[-1,0],[-1,-1])\n",
    "    distances = []\n",
    "    for a in actions:\n",
    "        m.set_state(current_state)\n",
    "        new_state = m.test_move(a, 0)\n",
    "        distances.append(manhattan_distance(new_state))\n",
    "    index = distances.index(min(distances))\n",
    "    return actions[index]\n",
    "    '''\n",
    "    t = []\n",
    "    for i in range(len(distances)):\n",
    "        if distances[i] == min_d:\n",
    "            t.append(actions[i])\n",
    "    t = sorted(t, key=lambda x: t[1])\n",
    "    return t[-1]\n",
    "    '''\n",
    "    \n",
    "\n",
    "states = []\n",
    "p0_action_state_dict = {}\n",
    "for i in range(m.length):\n",
    "    for j in range(m.width):\n",
    "        for h in range(12):\n",
    "            s = State(i,j,h)\n",
    "            states.append(s)\n",
    "            \n",
    "print(len(states))\n",
    "\n",
    "for s in states:\n",
    "    a = initial_policy(s)\n",
    "    p0_action_state_dict[(s.x,s.y,s.h)] = a\n",
    "\n",
    "def plot_trajectory(initial_state, p_error, policy_dict):\n",
    "    s = initial_state\n",
    "    m.set_state(initial_state)\n",
    "    states = []\n",
    "    steps = 100\n",
    "    while steps > 0:\n",
    "        if s.x == goal_state.x and s.y == goal_state.y:\n",
    "            print(\"Foudn goal!\")\n",
    "            break\n",
    "        else:\n",
    "            action = policy_dict[(s.x,s.y,s.h)]\n",
    "            s = m.move(action)\n",
    "            m.set_state(s)\n",
    "            states.append(s)\n",
    "            steps -= 1\n",
    "    return states\n",
    "\n",
    "states = plot_trajectory(State(3,3,0), 0, p0_action_state_dict)\n",
    "for s in states:\n",
    "    s.print_state()\n",
    "    \n",
    "#def policy_evaluation(input_state, discount_factor):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value iteration Question 4\n",
    "import copy\n",
    "\n",
    "def value_distance(V1, V2):\n",
    "    diff = 0\n",
    "    for k in V1:\n",
    "        diff += abs(V1[k] - V2[k])\n",
    "    return diff\n",
    "    \n",
    "\n",
    "V = {}\n",
    "states = []\n",
    "for i in range(m.length):\n",
    "    for j in range(m.width):\n",
    "        for h in range(12):\n",
    "            V[(i,j,h)] = 0\n",
    "            states.append(State(i,j,h))\n",
    "\n",
    "def value_iteration(gamma, p_error):\n",
    "    m.p_error = p_error\n",
    "    step = 0\n",
    "    while step < 1000:\n",
    "        V_prev = copy.deepcopy(V)\n",
    "        for s in states:\n",
    "            actions = ([1,1],[1,0],[1,-1],[-1,1],[-1,0],[-1,-1],[0,0])\n",
    "            action_score = []\n",
    "            for a in actions:\n",
    "                score = np.sum([p*(r + gamma*V_prev[(n_s.x, n_s.y, n_s.h)]) for n_s, p, r in m.get_next_state_probabilities(p_error, s, a)])\n",
    "                action_score.append(score)\n",
    "            max_score=max(action_score)\n",
    "            V[(s.x, s.y, s.h)] = max_score\n",
    "        print(value_distance(V_prev, V))\n",
    "        if abs(value_distance(V_prev, V)) < 0.001:\n",
    "            return V, V_prev\n",
    "        step += 1\n",
    "        print(step)\n",
    "\n",
    "#V, V_prev = value_iteration(0.9, 0.25)\n",
    "\n",
    "                               \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-203.84303927793366\n",
      "-203.84304048929553\n"
     ]
    }
   ],
   "source": [
    "print(V[0,1,0])\n",
    "print(V_prev[0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_from_value(V, gamma, p_error):\n",
    "    #empty policy\n",
    "    #for every state\n",
    "        #for every action\n",
    "            #score from optimal value func\n",
    "    policy={}\n",
    "    states = []\n",
    "    for i in range(m.length):\n",
    "        for j in range(m.width):\n",
    "            for h in range(12):\n",
    "                states.append(State(i,j,h))\n",
    "    \n",
    "    for s in states:\n",
    "        actions = ([1,1],[1,0],[1,-1],[-1,1],[-1,0],[-1,-1],[0,0])\n",
    "        action_score = []\n",
    "        for a in actions:\n",
    "            score = np.sum([p*(r + gamma*V[(n_s.x, n_s.y, n_s.h)]) for n_s, p, r in m.get_next_state_probabilities(p_error, s, a)])\n",
    "            action_score.append(score)\n",
    "        max_index = action_score.index(max(action_score))\n",
    "        best_action = actions[max_index]\n",
    "        policy[(s.x,s.y,s.h)] = best_action\n",
    "    return policy\n",
    "\n",
    "#trajectory = plot_trajectory(State(1,6,6), 0, policy_from_value(V, 0.9, 0))       \n",
    "#for t in trajectory:\n",
    "#    t.print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, gamma, p_error):\n",
    "    \n",
    "    V = {}\n",
    "    states = []\n",
    "    step = 0\n",
    "    for i in range(m.length):\n",
    "        for j in range(m.width):\n",
    "            for h in range(12):\n",
    "                V[(i,j,h)] = 0\n",
    "                states.append(State(i,j,h))\n",
    "    while step < 1000:\n",
    "        V_prev = copy.deepcopy(V)\n",
    "        for s in states:\n",
    "            action = policy[(s.x,s.y,s.h)]\n",
    "            V[(s.x,s.y,s.h)] = np.sum([p*(r + gamma*V_prev[(n_s.x, n_s.y, n_s.h)]) for n_s, p, r in m.get_next_state_probabilities(p_error, s, action)])\n",
    "        #print(value_distance(V_prev, V))\n",
    "        if abs(value_distance(V_prev, V)) < 0.001:\n",
    "            return V, V_prev\n",
    "        step += 1\n",
    "        #print(step)\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V, V_prev = policy_evaluation(p0_action_state_dict, 0.9, 0.25)\n",
    "#print(V[0,1,0])\n",
    "#print(V_prev[0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_difference(P1, P2):\n",
    "    for k in P1:\n",
    "        a1 = P1[k]\n",
    "        a2 = P2[k]\n",
    "        if a1[0] == a2[0] and a1[1] == a2[1]:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def policy_iteration(gamma, p_error):\n",
    "    policy = copy.deepcopy(p0_action_state_dict)\n",
    "    step = 0\n",
    "    while True:\n",
    "        V,_ = policy_evaluation(policy, gamma, p_error)\n",
    "        #print(len(list(V.keys())))\n",
    "        #print(\"Done with V\")\n",
    "        policy_new = policy_from_value(V, gamma, p_error)\n",
    "        #print(\"Finsihed getting new policy\")\n",
    "        if policy_difference(policy, policy_new):\n",
    "            return policy\n",
    "        else:\n",
    "            policy = copy.deepcopy(policy_new)\n",
    "            step += 1\n",
    "            print(step)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Foudn goal!\n",
      "State x,y,h is: 1,5,7\n",
      "State x,y,h is: 1,6,6\n",
      "State x,y,h is: 1,5,7\n",
      "State x,y,h is: 2,5,7\n",
      "State x,y,h is: 1,5,8\n",
      "State x,y,h is: 2,5,8\n",
      "State x,y,h is: 1,5,8\n",
      "State x,y,h is: 2,5,7\n",
      "State x,y,h is: 2,4,7\n",
      "State x,y,h is: 2,3,8\n",
      "State x,y,h is: 3,3,9\n",
      "State x,y,h is: 4,3,8\n",
      "State x,y,h is: 5,3,7\n",
      "State x,y,h is: 5,4,6\n",
      "State x,y,h is: 5,5,6\n",
      "State x,y,h is: 5,6,8\n"
     ]
    }
   ],
   "source": [
    "policy = policy_iteration(0.9, 0.25)\n",
    "m.p_error = 0.25\n",
    "trajectory = plot_trajectory(State(1,6,6), 0.25, policy)       \n",
    "for t in trajectory:\n",
    "    t.print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
