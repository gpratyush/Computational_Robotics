{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bA-yypRNeAZm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "class State:\n",
    "    #State is a class == x,y,h. x is x-cordinate, y is y-cordinate and h is the orientation or header.\n",
    "    def  __init__(self, x, y, h):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.h = h\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        self.x = state.x\n",
    "        self.y = state.y\n",
    "        self.h = state.h\n",
    "        \n",
    "    def equals(self, new_state):\n",
    "        if self.x == new_state.x and self.y == new_state.y and self.h == new_state.h:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def print_state(self):\n",
    "        print(\"State x,y,h is: \" + str(self.x) + \",\" + str(self.y)+  \",\" + str(self.h))\n",
    "\n",
    "\n",
    "\n",
    "class Matrix:\n",
    "    # our complete grid \n",
    "    # includes dimensions, and structure\n",
    "    # includes movement \n",
    "    # includes transition probability function\n",
    "    def __init__(self, p_error, state, length=8, width=8):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.p_error = p_error\n",
    "        self.state = state\n",
    "        self.vertical_positive = [11,0,1]\n",
    "        self.vertical_negative = [5,6,7]\n",
    "        self.horizontal_positive = [2,3,4]\n",
    "        self.horizontal_negative = [10,9,8]\n",
    "        self.reward_dict = {} #dictionary\n",
    "        state_space = []\n",
    "        for i in range(m.length):\n",
    "            for j in range(m.width):\n",
    "                for h in range(12):\n",
    "                    s = State(i,j,h)\n",
    "                    state.append(s)\n",
    "        self.statelist = state_space\n",
    "       \n",
    "    def set_state(self, new_state):\n",
    "        self.state = new_state\n",
    "\n",
    "    # returns pre rotation change -1 or 0 or 1 based on probability p_e\n",
    "    def pre_rotation(self):\n",
    "        v = random.random()\n",
    "        #print(\"The pre rotation probability: \" + str(v))\n",
    "        if v < self.p_error:\n",
    "            return -1\n",
    "        elif v >= self.p_error and v < 2*self.p_error:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    #This method is only used as a helper to caluclate the probabilty of the next state given the input\n",
    "    #state and action. \n",
    "    def test_move(self, action, error):\n",
    "        h = (self.state.h + error) % 12\n",
    "        multiplier = 1\n",
    "        if h in self.horizontal_negative or h in self.vertical_negative:\n",
    "            multiplier = -1\n",
    "        new_x = self.state.x\n",
    "        new_y = self.state.y\n",
    "        if h in self.horizontal_negative or h in self.horizontal_positive:\n",
    "            temp_x = self.state.x + (multiplier*action[0])\n",
    "            if 0 <= temp_x <= self.length - 1:\n",
    "                new_x += (multiplier*action[0])\n",
    "        else:\n",
    "            temp_y = self.state.y + (multiplier*action[0])\n",
    "            if 0 <= temp_y <= self.width - 1:\n",
    "                new_y += (multiplier*action[0])\n",
    "        new_h = (h + action[1]) % 12\n",
    "        return State(new_x, new_y, new_h)\n",
    "\n",
    "    #answer to ques: 1)c)(using test_move) probability function (transfer function)\n",
    "    #Calculates the probability of new_state given initial state and action.\n",
    "    def probability_new_state(self, input_action, new_state):\n",
    "        if input_action[0] == 0:\n",
    "            if self.state.equals(new_state):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            probability = 0\n",
    "            errors = [-1, 0, 1]\n",
    "            for e in errors:\n",
    "                result_state = self.test_move(input_action, e)\n",
    "                if new_state.equals(result_state):\n",
    "                    #if new_x == result_state[0] and new_y == result_state[1] and new_h == result_state[2]:\n",
    "                    if e == 1 or e == -1:\n",
    "                        probability += self.p_error\n",
    "                    else:\n",
    "                        probability += (1 - 2*self.p_error)\n",
    "            return probability\n",
    "        \n",
    "    #returns the next given input state, action and the p_error.\n",
    "    #ans to 1)d)    \n",
    "    def move(self, action):\n",
    "        h = (self.state.h + self.pre_rotation()) % 12\n",
    "        multiplier = 1\n",
    "        if h in self.horizontal_negative or h in self.vertical_negative:\n",
    "            multiplier = -1\n",
    "        new_x = self.state.x\n",
    "        new_y = self.state.y\n",
    "        if h in self.horizontal_negative or h in self.horizontal_positive:\n",
    "            temp_x = self.state.x + (multiplier*action[0])\n",
    "            if 0 <= temp_x <= self.length - 1:\n",
    "                new_x += (multiplier*action[0])\n",
    "        else:\n",
    "            temp_y = self.state.y + (multiplier*action[0])\n",
    "            if 0 <= temp_y <= self.width - 1:\n",
    "                new_y += (multiplier*action[0])\n",
    "        new_h = (h + action[1]) % 12\n",
    "        return State(new_x, new_y, new_h)\n",
    "\n",
    "    #returns list of tuples (next_state, Probability of next state, reward of this transition) for\n",
    "    #all possible states that are reachable from a given state. Takes into account p_error.\n",
    "    def get_next_state_probabilities(self, initial_state, action):\n",
    "        return_value = []\n",
    "        self.set_state(initial_state)\n",
    "        new_states = []\n",
    "        x,y,h = initial_state.x, initial_state.y, initial_state.h\n",
    "        positions = [(x,y), (x-1,y), (x+1,y), (x,y-1), (x,y+1)]\n",
    "        new_positions = []\n",
    "        for p in positions:\n",
    "            if 0 <= p[0] <= self.length - 1 and 0 <= p[1] <= self.width - 1:\n",
    "                new_positions.append((p[0],p[1]))\n",
    "        new_headings = [(h-2)%12, (h-1)%12, h, (h+1)%12, (h+2)%12]\n",
    "        for p in new_positions:\n",
    "            for o in new_headings:\n",
    "                new_states.append(State(p[0], p[1], o))\n",
    "        probabilities = []\n",
    "        for s in new_states:\n",
    "            prob = self.probability_new_state(action, s)\n",
    "            return_value.append((s, prob, self.get_reward(s)))\n",
    "        return return_value\n",
    "    \n",
    "    #Given co-ordinates, returns the reward.        \n",
    "    def get_reward(self, input_state):\n",
    "        x,y = input_state.x, input_state.y\n",
    "        return self.reward_dict[(x,y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A05Byx2gi3hS"
   },
   "outputs": [],
   "source": [
    "#so define m = p_error, inital state(x ,y, h) \n",
    "# then m.reward_dict= reward dictionary and you're set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqaXfyHkeAaW"
   },
   "outputs": [],
   "source": [
    "#This part deals with Question 2 and setting the reward map.\n",
    "\n",
    "goal_state = State(5,6,None)\n",
    "def make_reward_map():\n",
    "    d = {}\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if i == 7 or j == 7:\n",
    "                d[(i,j)] = -100\n",
    "            elif i == 0 or j == 0:\n",
    "                d[(i,j)] = -100\n",
    "            elif i == goal_state.x and j == goal_state.y:\n",
    "                d[(i,j)] = 1\n",
    "            else:\n",
    "                d[(i,j)] = 0\n",
    "    d[(3,6)] = -10\n",
    "    d[(3,5)] = -10\n",
    "    d[(3,4)] = -10\n",
    "    return d\n",
    "\n",
    "p_error=0\n",
    "state=State(0,0,0)\n",
    "m=Matrix(p_error,state)\n",
    "m.reward_dict = make_reward_map()\n",
    "#you're set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "K0ia0XjieAak",
    "outputId": "b5c2b220-c17a-4a72-d914-1db3b2bee8e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found goal!\n",
      "State x,y,h is: 3,4,1\n",
      "State x,y,h is: 3,5,2\n",
      "State x,y,h is: 4,5,3\n",
      "State x,y,h is: 5,5,4\n",
      "State x,y,h is: 6,5,5\n",
      "State x,y,h is: 6,6,6\n",
      "State x,y,h is: 6,5,7\n",
      "State x,y,h is: 6,6,8\n",
      "State x,y,h is: 5,6,9\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "#Algorithm for Part 3 initial question. We are using Manhattan distance as the metric. The algorithm just \n",
    "#returns the action which corresponds to the lowest Manhattan distance out of 6 possible actions.\n",
    "#We use 6 possible actions and not 7 because unless the agent is at the goal state, it is forced to move.\n",
    "\n",
    "def manhattan_distance(current_state):\n",
    "    return abs(current_state.x - goal_state.x) + abs(current_state.y - goal_state.y)\n",
    "\n",
    "#gives initial policy as the action that minimizes the remaining Manhattan distance.\n",
    "def initial_policy(current_state):\n",
    "    if current_state.x == goal_state.x and current_state.y == goal_state.y:\n",
    "        return [0,0]\n",
    "    actions = ([1,1],[1,0],[1,-1],[-1,1],[-1,0],[-1,-1])\n",
    "    distances = []\n",
    "    for a in actions:\n",
    "        m.set_state(current_state)\n",
    "        new_state = m.test_move(a, 0)\n",
    "        distances.append(manhattan_distance(new_state))\n",
    "    index = distances.index(min(distances))\n",
    "    return actions[index]   \n",
    "\n",
    "init_policy = {}\n",
    "\n",
    "for s in m.statelist:\n",
    "    a = initial_policy(s)\n",
    "    init_policy[(s.x,s.y,s.h)] = a\n",
    "\n",
    "#we have an initial policy\n",
    "\n",
    "#Give an initial state and a policy dictionary, the function outputs the states that the agent has\n",
    "#been in on its way to the goal state. If the agent takes more than 100 steps, the method exits. This \n",
    "#limit can be changed by the user.\n",
    "def plot_trajectory(initial_state, policy_dict):\n",
    "    s = initial_state\n",
    "    m.set_state(initial_state)\n",
    "    states = []\n",
    "    steps = 100\n",
    "    while steps > 0:\n",
    "        if s.x == goal_state.x and s.y == goal_state.y:\n",
    "            print(\"Found goal!\")\n",
    "            break\n",
    "        else:\n",
    "            action = policy_dict[(s.x,s.y,s.h)]\n",
    "            s = m.move(action)\n",
    "            m.set_state(s)\n",
    "            states.append(s)\n",
    "            steps -= 1\n",
    "    return states\n",
    "\n",
    "states = plot_trajectory(State(3,3,0), init_policy)\n",
    "for s in states:\n",
    "    s.print_state() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alrQtBcYeAaw"
   },
   "outputs": [],
   "source": [
    "#Value iteration Question 4a\n",
    "import copy\n",
    "\n",
    "def value_distance(V1, V2):\n",
    "    diff = 0\n",
    "    for k in V1:\n",
    "        diff += abs(V1[k] - V2[k])\n",
    "    return diff\n",
    "    \n",
    "\n",
    "V = {}\n",
    "for s in m.statelist:\n",
    "    V[(s.x,s.y,s.h)]=0\n",
    "\n",
    "def value_iteration(gamma, p_error):\n",
    "    m.p_error = p_error\n",
    "    step = 0\n",
    "    while step < 1000:\n",
    "        V_prev = copy.deepcopy(V)\n",
    "        for s in m.statelist:\n",
    "            actions = ([1,1],[1,0],[1,-1],[-1,1],[-1,0],[-1,-1],[0,0])\n",
    "            action_score = []\n",
    "            for a in actions:\n",
    "                score = np.sum([p*(r + gamma*V_prev[(n_s.x, n_s.y, n_s.h)]) for n_s, p, r in m.get_next_state_probabilities(s, a)])\n",
    "                action_score.append(score)\n",
    "            max_score=max(action_score)\n",
    "            V[(s.x, s.y, s.h)] = max_score\n",
    "        print(value_distance(V_prev, V))\n",
    "        if abs(value_distance(V_prev, V)) < 0.001:\n",
    "            return V, V_prev\n",
    "        step += 1\n",
    "        print(step)                       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "eGc5PdV5eAa4",
    "outputId": "242ec6fa-4328-49a4-e4ea-5240c4679446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-203.84303927793366\n",
      "-203.84304048929553\n"
     ]
    }
   ],
   "source": [
    "#We ptinted these two values just to see the difference between the values after the value iteration.\n",
    "print(V[0,1,0])\n",
    "print(V_prev[0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "65-Ti__EeAbB",
    "outputId": "2e93908f-f267-4ec6-d7be-512c7fa82082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19404.0\n",
      "1\n",
      "9613.299999999996\n",
      "2\n",
      "4217.520937500001\n",
      "3\n",
      "1782.0816328124984\n",
      "4\n",
      "807.6354152343737\n",
      "5\n",
      "374.64552764648374\n",
      "6\n",
      "203.27684886145002\n",
      "7\n",
      "140.8675834348858\n",
      "8\n",
      "132.5878392014197\n",
      "9\n",
      "135.79196459128562\n",
      "10\n",
      "137.0481522933456\n",
      "11\n",
      "135.7260845998269\n",
      "12\n",
      "131.95553842502792\n",
      "13\n",
      "126.47321187959247\n",
      "14\n",
      "119.79113685570056\n",
      "15\n",
      "112.61698511484605\n",
      "16\n",
      "105.26701591375817\n",
      "17\n",
      "97.94582645381043\n",
      "18\n",
      "90.67391380044262\n",
      "19\n",
      "83.7216105798792\n",
      "20\n",
      "76.93385941297913\n",
      "21\n",
      "70.4681834657564\n",
      "22\n",
      "64.33313666120455\n",
      "23\n",
      "58.61841372611514\n",
      "24\n",
      "53.28724759420554\n",
      "25\n",
      "48.375021402621\n",
      "26\n",
      "43.84570829363446\n",
      "27\n",
      "39.70361999635763\n",
      "28\n",
      "36.000648605059816\n",
      "29\n",
      "32.58664934588541\n",
      "30\n",
      "29.469894618888873\n",
      "31\n",
      "26.634330172500693\n",
      "32\n",
      "24.086929901702533\n",
      "33\n",
      "21.77525338902951\n",
      "34\n",
      "19.65745683869106\n",
      "35\n",
      "17.741378588615934\n",
      "36\n",
      "16.005051070048967\n",
      "37\n",
      "14.433350681403322\n",
      "38\n",
      "13.012871684177426\n",
      "39\n",
      "11.744422438658429\n",
      "40\n",
      "10.609052598932923\n",
      "41\n",
      "9.575946646634108\n",
      "42\n",
      "8.634368549901687\n",
      "43\n",
      "7.780220625559387\n",
      "44\n",
      "7.007967476140223\n",
      "45\n",
      "6.311073029338425\n",
      "46\n",
      "5.685797947804408\n",
      "47\n",
      "5.122743107848457\n",
      "48\n",
      "4.614275871364092\n",
      "49\n",
      "4.159915125778283\n",
      "50\n",
      "3.7535801555733004\n",
      "51\n",
      "3.3854520430620134\n",
      "52\n",
      "3.050322272728209\n",
      "53\n",
      "2.7469713015715866\n",
      "54\n",
      "2.473197697358471\n",
      "55\n",
      "2.226552912449404\n",
      "56\n",
      "2.00432545247859\n",
      "57\n",
      "1.8041825921079102\n",
      "58\n",
      "1.623961929339349\n",
      "59\n",
      "1.4617138297434038\n",
      "60\n",
      "1.3156465573750307\n",
      "61\n",
      "1.1841612276490703\n",
      "62\n",
      "1.0658011667071898\n",
      "63\n",
      "0.9592639451999831\n",
      "64\n",
      "0.8633679966030745\n",
      "65\n",
      "0.7770545181445545\n",
      "66\n",
      "0.6993656582221641\n",
      "67\n",
      "0.629441813509656\n",
      "68\n",
      "0.5665066988217622\n",
      "69\n",
      "0.509862984696623\n",
      "70\n",
      "0.45888165128189096\n",
      "71\n",
      "0.4129972970387801\n",
      "72\n",
      "0.37170029123758086\n",
      "73\n",
      "0.3345323536430782\n",
      "74\n",
      "0.3010806151105694\n",
      "75\n",
      "0.2709737033344566\n",
      "76\n",
      "0.24387715679832844\n",
      "77\n",
      "0.21949007409158705\n",
      "78\n",
      "0.1975415207258555\n",
      "79\n",
      "0.1777877176281999\n",
      "80\n",
      "0.1600091964615098\n",
      "81\n",
      "0.14400846947714863\n",
      "82\n",
      "0.1296077610215023\n",
      "83\n",
      "0.11664709142256058\n",
      "84\n",
      "0.10498245891452607\n",
      "85\n",
      "0.09448427197246456\n",
      "86\n",
      "0.0850358872310184\n",
      "87\n",
      "0.07653233117459468\n",
      "88\n",
      "0.06887912160490339\n",
      "89\n",
      "0.06199122756813846\n",
      "90\n",
      "0.055792117885427484\n",
      "91\n",
      "0.05021291616277024\n",
      "92\n",
      "0.04519163181349661\n",
      "93\n",
      "0.04067247422887066\n",
      "94\n",
      "0.036605230849096504\n",
      "95\n",
      "0.03294471087943851\n",
      "96\n",
      "0.029650242042804198\n",
      "97\n",
      "0.026685219571733754\n",
      "98\n",
      "0.024016698870902076\n",
      "99\n",
      "0.021615029951041365\n",
      "100\n",
      "0.019453527656010827\n",
      "101\n",
      "0.01750817543028882\n",
      "102\n",
      "0.015757358277974642\n",
      "103\n",
      "0.014181622751427092\n",
      "104\n",
      "0.01276346069429346\n",
      "105\n",
      "0.011487114793963915\n",
      "106\n",
      "0.010338403436362942\n",
      "107\n",
      "0.009304563187897408\n",
      "108\n",
      "0.00837410693626417\n",
      "109\n",
      "0.007536696295362244\n",
      "110\n",
      "0.006783026703906625\n",
      "111\n",
      "0.006104724064315281\n",
      "112\n",
      "0.005494251678217044\n",
      "113\n",
      "0.004944826527270685\n",
      "114\n",
      "0.004450343886626129\n",
      "115\n",
      "0.004005309507526089\n",
      "116\n",
      "0.0036047785633068763\n",
      "117\n",
      "0.0032443007115872113\n",
      "118\n",
      "0.0029198706443951394\n",
      "119\n",
      "0.0026278835832340697\n",
      "120\n",
      "0.002365095227321312\n",
      "121\n",
      "0.002128585705296171\n",
      "122\n",
      "0.0019157271368883677\n",
      "123\n",
      "0.0017241544234463113\n",
      "124\n",
      "0.001551738982328743\n",
      "125\n",
      "0.001396565084453627\n",
      "126\n",
      "0.001256908575719251\n",
      "127\n",
      "0.0011312177188629313\n",
      "128\n",
      "0.0010180959476224771\n",
      "129\n",
      "0.0009162863529557086\n",
      "Found goal!\n",
      "State x,y,h is: 1,5,5\n",
      "State x,y,h is: 1,4,6\n",
      "State x,y,h is: 1,3,5\n",
      "State x,y,h is: 1,2,4\n",
      "State x,y,h is: 2,2,5\n",
      "State x,y,h is: 2,3,4\n",
      "State x,y,h is: 3,3,4\n",
      "State x,y,h is: 3,2,4\n",
      "State x,y,h is: 4,2,4\n",
      "State x,y,h is: 5,2,5\n",
      "State x,y,h is: 5,3,7\n",
      "State x,y,h is: 6,3,7\n",
      "State x,y,h is: 6,2,6\n",
      "State x,y,h is: 6,3,5\n",
      "State x,y,h is: 6,4,6\n",
      "State x,y,h is: 6,5,6\n",
      "State x,y,h is: 6,6,7\n",
      "State x,y,h is: 6,5,7\n",
      "State x,y,h is: 6,4,6\n",
      "State x,y,h is: 6,5,4\n",
      "State x,y,h is: 5,5,5\n",
      "State x,y,h is: 5,6,5\n"
     ]
    }
   ],
   "source": [
    "#Given the list of values for each state and a discount factor we get the policy associated with \n",
    "#these parameters. Answer for 3f\n",
    "def policy_from_value(V, gamma):\n",
    "    policy={}    \n",
    "    for s in m.statelist:\n",
    "        actions = ([1,1],[1,0],[1,-1],[-1,1],[-1,0],[-1,-1],[0,0])\n",
    "        action_score = []\n",
    "        for a in actions:\n",
    "            score = np.sum([p*(r + gamma*V[(n_s.x, n_s.y, n_s.h)]) for n_s, p, r in m.get_next_state_probabilities(s, a)])\n",
    "            action_score.append(score)\n",
    "        max_index = action_score.index(max(action_score))\n",
    "        best_action = actions[max_index]\n",
    "        policy[(s.x,s.y,s.h)] = best_action\n",
    "    return policy\n",
    "\n",
    "V, V_prev = value_iteration(0.9, 0.25)    \n",
    "trajectory = plot_trajectory(State(1,6,6), policy_from_value(V, 0.9))       \n",
    "for t in trajectory:\n",
    "    t.print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEoMf_CleAbK"
   },
   "outputs": [],
   "source": [
    "#Answer for 3d.\n",
    "def policy_evaluation(policy, gamma):\n",
    "    \n",
    "    V = {}\n",
    "    for s in m.statelist:\n",
    "        V[(s.x,s.y,s.h)]=0\n",
    "    step = 0\n",
    "    while step < 1000:\n",
    "        V_prev = copy.deepcopy(V)\n",
    "        for s in m.statelist:\n",
    "            action = policy[(s.x,s.y,s.h)]\n",
    "            V[(s.x,s.y,s.h)] = np.sum([p*(r + gamma*V_prev[(n_s.x, n_s.y, n_s.h)]) for n_s, p, r in m.get_next_state_probabilities(s, action)])\n",
    "        #print(value_distance(V_prev, V))\n",
    "        if abs(value_distance(V_prev, V)) < 0.001:\n",
    "            return V, V_prev\n",
    "        step += 1\n",
    "        #print(step)\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "fiNnQLb7eAbU",
    "outputId": "df60e6ac-ef66-4b1f-933f-6df18224bf7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-265.4565267588724\n",
      "-265.45652800994594\n"
     ]
    }
   ],
   "source": [
    "V, V_prev = policy_evaluation(init_policy, 0.9)\n",
    "print(V[0,1,0])\n",
    "print(V_prev[0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJfgLIDueAbg"
   },
   "outputs": [],
   "source": [
    "def policy_difference(P1, P2):\n",
    "    for k in P1:\n",
    "        a1 = P1[k]\n",
    "        a2 = P2[k]\n",
    "        if a1[0] == a2[0] and a1[1] == a2[1]:\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "#Answer for 3g\n",
    "def policy_iteration(init_policy, gamma, p_error):\n",
    "    m.p_error=p_error\n",
    "    policy = copy.deepcopy(init_policy)\n",
    "    step = 0\n",
    "    while True:\n",
    "        V,_ = policy_evaluation(policy, gamma)\n",
    "        #print(len(list(V.keys())))\n",
    "        #print(\"Done with V\")\n",
    "        policy_new = policy_from_value(V, gamma)\n",
    "        #print(\"Finsihed getting new policy\")\n",
    "        if policy_difference(policy, policy_new):\n",
    "            return policy\n",
    "        else:\n",
    "            policy = copy.deepcopy(policy_new)\n",
    "            step += 1\n",
    "            print(step)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "colab_type": "code",
    "id": "ncj9lGubeAbs",
    "outputId": "14da3118-e741-4849-eb02-dcbd116d240b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Found goal!\n",
      "State x,y,h is: 1,5,6\n",
      "State x,y,h is: 1,4,5\n",
      "State x,y,h is: 1,3,4\n",
      "State x,y,h is: 1,2,4\n",
      "State x,y,h is: 2,2,5\n",
      "State x,y,h is: 2,3,5\n",
      "State x,y,h is: 2,2,5\n",
      "State x,y,h is: 1,2,3\n",
      "State x,y,h is: 2,2,2\n",
      "State x,y,h is: 3,2,3\n",
      "State x,y,h is: 4,2,2\n",
      "State x,y,h is: 5,2,2\n",
      "State x,y,h is: 6,2,2\n",
      "State x,y,h is: 5,2,2\n",
      "State x,y,h is: 5,3,0\n",
      "State x,y,h is: 5,4,1\n",
      "State x,y,h is: 6,4,1\n",
      "State x,y,h is: 6,3,0\n",
      "State x,y,h is: 6,4,0\n",
      "State x,y,h is: 6,5,11\n",
      "State x,y,h is: 5,5,11\n",
      "State x,y,h is: 4,5,10\n",
      "State x,y,h is: 4,4,0\n",
      "State x,y,h is: 4,5,1\n",
      "State x,y,h is: 4,6,0\n",
      "State x,y,h is: 4,5,1\n",
      "State x,y,h is: 4,6,0\n",
      "State x,y,h is: 4,5,2\n",
      "State x,y,h is: 5,5,1\n",
      "State x,y,h is: 5,6,1\n"
     ]
    }
   ],
   "source": [
    "policy = policy_iteration(init_policy, 0.9, 0.25)\n",
    "trajectory = plot_trajectory(State(1,6,6), policy)       \n",
    "for t in trajectory:\n",
    "    t.print_state()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mdp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
